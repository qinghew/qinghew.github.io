<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Qinghe Wangï¼ˆç‹æ¸…å’Œï¼‰| Dalian University of Technologyï¼ˆå¤§è¿ç†å·¥å¤§å­¦ï¼‰</title>
</head>
<body>
<div id="layout-content">
<div id="toptitle">
<h1>Qinghe Wang (ç‹æ¸…å’Œ)</h1>
</div>
<table class="imgtable"><tr><td>
<a href="http://qinghew.github.io"><img src="indexpic/wqh.jpg" alt="alt text" width="150px" height="180px" /></a>&nbsp;</td>
<td align="left"><p>I am a third-year PhD student at the IIAU-Lab in Dalian University of Technology, under the supervision of Prof. <a href="https://scholar.google.com/citations?user=D3nE0agAAAAJ&amp;hl=zh-CN">Huchuan Lu (IEEE Fellow)</a> and Prof. <a href="https://stephenjia.github.io/">Xu Jia</a>. I obtained my Master's degree from Tianjin University supervised by Prof. <a href="http://aiskyeye.com/">Pengfei Zhu</a> and Prof. <a href="https://bcaosudo.github.io/">Bing Cao</a>. I obtained my Bachelor's degree from Dalian Maritime University. My research focuses on Video/Image generation with Diffusion models/GANs.</p>
<p>E-mail: aitwqh@163.com <br /></p>
<p>[<a href="https://github.com/qinghew">GitHub</a>] [<a href="https://scholar.google.com/citations?user=bY5ueZsAAAAJ&amp;hl=zh-CN">Google Scholar</a>] [<a href="https://x.com/home">Twitter/X</a>]</p>
</td></tr></table>
<h2>News!</h2>
<p>[2025/2]&nbsp;&nbsp;Release <a href="https://github.com/KlingAIResearch/MultiShotMaster"><b>Codes</b></a> of "MultiShotMaster". Welcome to use! <br />
[2025/12]&nbsp;&nbsp;Release paper "<a href="https://qinghew.github.io/MultiShotMaster/">MultiShotMaster: A Controllable Multi-Shot Video Generation Framework</a>". <br />
[2025/10]&nbsp;&nbsp;Release paper "<a href="https://libaolu312.github.io/VFXMaster/">VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context Learning</a>". <br />
[2025/2]&nbsp;&nbsp;Release paper "<a href="https://arxiv.org/abs/2502.08639">CineMaster: A 3D-Aware and Controllable Framework for Cinematic Text-to-Video Generation</a>". <br />
[2024/5]&nbsp;&nbsp;Release a <a href="https://huggingface.co/spaces/Qinghew/CharacterFactory"><b>Gradio Demo</b></a> of "CharacterFactory". Welcome to play! <br />
[2024/5]&nbsp;&nbsp;Release all the <a href="https://github.com/qinghew/CharacterFactory"><b>Codes</b></a> of "CharacterFactory". Welcome to use! <br />
[2024/4]&nbsp;&nbsp;Release paper "<a href="https://qinghew.github.io/CharacterFactory">CharacterFactory: Sampling Consistent Characters with GANs for Diffusion Models</a>". <br />
[2024/3]&nbsp;&nbsp;Release all the <a href="https://github.com/qinghew/StableIdentity"><b>Codes</b></a> of "StableIdentity". Welcome to use! <br />
[2024/1]&nbsp;&nbsp;Release paper "<a href="https://qinghew.github.io/StableIdentity">StableIdentity: Inserting Anybody into Anywhere at First Sight</a>", <a href="[https://x.com/ylecun/status/1752545984352080201?s=20"><b>LeCun: OMG</b></a>. <br /></p>
<h2>Internships</h2>
<table class="imgtable"><tr><td>
<a href="indexpic\kuaishou.png"><img src="indexpic\kuaishou.png" alt="alt text" width="200px" height="40px" /></a>&nbsp;</td>
<td align="left"><p><b>Kuaishou</b><br />
2024.7~Now Research Intern in Kling Team, Kuaishou Technology (Shenzhen)&nbsp;&nbsp;<br />
Supervisors: <a href="https://xiaoyushi97.github.io/">Xiaoyu Shi</a>, <a href="https://xinntao.github.io/">Xintao Wang</a> <br />
Topic: Video Generation</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<a href="indexpic\bytedance.png"><img src="indexpic\bytedance.png" alt="alt text" width="200px" height="80px" /></a>&nbsp;</td>
<td align="left"><p><b>ByteDance</b><br />
2022.4~2022.12 Research Intern in ByteDance Intelligent Creation Lab (Beijing)&nbsp;&nbsp;<br />
Supervisors: <a href="https://liulj13.github.io/">Lijie Liu</a>, <a href="https://scholar.google.com/citations?user=9rWWCgUAAAAJ&amp;hl=zh-CN">Qian He</a> <br />
Topic: Image Generation</p>
</td></tr></table>
<h2>Selected Publications</h2>
<table class="imgtable"><tr><td>
<a href="indexpic\multishotmaster.gif"><img src="indexpic\multishotmaster.gif" alt="alt text" class="gif-image" width="400px" height="280px" /></a>&nbsp;</td>
<td align="left"><p>[<b>Arxiv 2025</b>]&nbsp;&nbsp;<br /><u>MultiShotMaster: A Controllable Multi-Shot Video Generation Framework
</u> <br /> <b>Qinghe Wang</b>, Xiaoyu Shiâœ‰, Baolu Li, Weikang Bian, Quande Liu, Huchuan Lu, Xintao Wang, Pengfei Wan, Kun Gai, Xu Jiaâœ‰ <br /> [<a href="https://arxiv.org/abs/2512.03041">Paper</a>] [<a href="https://qinghew.github.io/MultiShotMaster/">Project Page</a>] [<a href="https://github.com/KlingAIResearch/MultiShotMaster">Code</a>] [<a href="https://sites.google.com/view/aaai26-cvm/award-results">ğŸ¥‡ 1st Place at AAAI CVM 2026</a>] <br /><br />
The first controllable multi-shot video generation framework that supports text-driven inter-shot consistency, customized subject with motion control, and background-driven customized scene. Both shot counts and shot durations are variable. </p>
</td></tr></table>
<table class="imgtable"><tr><td>
<a href="indexpic\vfxmaster.gif"><img src="indexpic\vfxmaster.gif" alt="alt text" class="gif-image" width="400px" height="220px" /></a>&nbsp;</td>
<td align="left"><p>[<b>Arxiv 2025</b>]&nbsp;&nbsp;<br /><u>VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context Learning
</u> <br /> Baolu Li*, Yiming Zhang*, <b>Qinghe Wang</b>*(co-first, Project Lead), Liqian Maâœ‰, Xiaoyu Shi, Xintao Wang, Pengfei Wan, Zhenfei Yin, Yunzhi Zhuge, Huchuan Lu, Xu Jiaâœ‰ <br /> [<a href="https://arxiv.org/abs/2510.25772">Paper</a>] [<a href="https://libaolu312.github.io/VFXMaster/">Project Page</a>] <br /><br />
A unified reference-based VFX video generation framework allows users to reproduce diverse dynamic effects from reference videos onto target content.</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<a href="indexpic\cinemaster-teaser.gif"><img src="indexpic\cinemaster-teaser.gif" alt="alt text" class="gif-image" width="400px" height="240px" /></a>&nbsp;</td>
<td align="left"><p>[<b>SIGGRAPH 2025</b> <span style="color: blue"> CCF A</span>]&nbsp;&nbsp;<br /><u>CineMaster: A 3D-Aware and Controllable Framework for Cinematic Text-to-Video Generation
</u> <br /> <b>Qinghe Wang</b>*, Yawen Luo*(co-first), Xiaoyu Shiâœ‰, Xu Jiaâœ‰, Huchuan Lu, Tianfan Xueâœ‰, Xintao Wang, Pengfei Wan, Di Zhang, Kun Gai. <br /> [<a href="https://arxiv.org/abs/2502.08639">Paper</a>] [<a href="https://cinemaster-dev.github.io/">Project Page</a>] <br /><br />
A 3D-aware and controllable text-to-video generation method allows users to manipulate objects and camera jointly in 3D space for high-quality cinematic video creation.</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<a href="CharacterFactory\static\images\teaser.svg"><img src="CharacterFactory\static\images\teaser.svg" alt="alt text" width="400px" height="240px" /></a>&nbsp;</td>
<td align="left"><p>[<b>TIP 2025</b> <span style="color: blue"> CAS-JCR Q1 Top; CCF A</span>]&nbsp;&nbsp;<br /><u>CharacterFactory: Sampling Consistent Characters with GANs for Diffusion Models
</u> <br /> <b>Qinghe Wang</b>, Baolu Li, Xiaomin Li, Bing Cao, Liqian Ma, Huchuan Lu, Xu Jiaâœ‰. <br /> [<a href="https://huggingface.co/spaces/Qinghew/CharacterFactory"><b>Gradio Demo</b></a>] [<a href="https://arxiv.org/abs/2404.15677">Paper</a>] [<a href="https://qinghew.github.io/CharacterFactory/">Project Page</a>] [<a href="https://github.com/qinghew/CharacterFactory">Code</a>] <br /><br />
An end-to-end framework allows users to sample new characters with consistent identities for new character creation.</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<a href="indexpic\sdid_video.gif"><img src="indexpic\sdid_video.gif" alt="alt text" class="gif-image" width="400px" height="210px" /></a>&nbsp;</td>
<td align="left"><p>[<b>TMM 2025</b> <span style="color: blue"> CAS-JCR Q1 Top</span>]&nbsp;&nbsp;<br /><u>StableIdentity: Inserting Anybody into Anywhere at First Sight
</u> <br /> <b>Qinghe Wang</b>, Xu Jiaâœ‰, Xiaomin Li, Taiqing Li, Liqian Ma, Yunzhi Zhuge, Huchuan Lu. <br /> [<a href="https://arxiv.org/abs/2401.15975">Paper</a>] [<a href="https://qinghew.github.io/StableIdentity/">Project Page</a>] [<a href="https://github.com/qinghew/StableIdentity">Code</a>] [<a href="https://x.com/ylecun/status/1752545984352080201?s=20"><b>LeCun: OMG</b></a>] <br /><br /> 
A framework enables identity-consistent generation with just one face image, and seamless integration with video/3D generation without finetuning.</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<a href="indexpic/hs_diffusion.png"><img src="indexpic/hs_diffusion.png" alt="alt text" width="400px" height="280px" /></a>&nbsp;</td>
<td align="left"><p>[<b>Arxiv 2023</b>]&nbsp;&nbsp;<br /><u>HS-Diffusion: Semantic-Mixing Diffusion for Head Swapping
</u> <br /> <b>Qinghe Wang</b>, Lijie Liu, Miao Hua, Pengfei Zhu, Wangmeng Zuo, Qinghua Hu, Huchuan Lu, Bing Caoâœ‰. <br /> [<a href="https://arxiv.org/abs/2212.06458">Paper</a>] [<a href="https://github.com/qinghew/HS-Diffusion">Code</a>] <br /><br />
A framework enables seamless head swapping, preserving both head and body details while generating natural transition regions.</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<a href="indexpic/mvke.png"><img src="indexpic/mvke.png" alt="alt text" width="400px" height="220px" /></a>&nbsp;</td>
<td align="left"><p>[<b>TNNLS 2023</b> <span style="color: blue"> CAS-JCR Q1 Top</span>]&nbsp;&nbsp;<br /><u>Multi-view knowledge ensemble with frequency consistency for cross-domain face translation</u> <br /> Bing Cao, <b>Qinghe Wang</b>, Pengfei Zhuâœ‰, Qinghua Hu, Dongwei Ren, Wangmeng Zuo, Xinbo Gao. <br /> [<a href="https://ieeexplore.ieee.org/abstract/document/10021294">Paper</a>] [<a href="https://github.com/qinghew/MvKE-FC">Code</a>] <br /><br /> 
A framework enables cross-domain face translation with structural and identity preservation.</p>
</td></tr></table>
<table class="imgtable"><tr><td>
<a href="indexpic/scgan.png"><img src="indexpic/scgan.png" alt="alt text" width="400px" height="180px" /></a>&nbsp;</td>
<td align="left"><p>[<b>ä¸­å›½ç§‘å­¦ï¼šä¿¡æ¯ç§‘å­¦</b> <span style="color: blue">CCF Aç±»ä¸­æ–‡æœŸåˆŠ</span>]&nbsp;&nbsp;<br /><u>åŸºäºè‡ªåˆ¤åˆ«å¾ªç¯ç”Ÿæˆå¯¹æŠ—ç½‘ç»œçš„äººè„¸å›¾åƒç¿»è¯‘</u> (Nickname: CycleGAN--) <br /> ä¸­å›½ç§‘å­¦ï¼šä¿¡æ¯ç§‘å­¦ (2022). <br /> <b>ç‹æ¸…å’Œ</b>, æ›¹å…µâœ‰, æœ±é¹é£, ç‹æ¥ æ¥ , èƒ¡æ¸…å, é«˜æ–°æ³¢. <br /> [<a href="https://www.sciengine.com/SSI/doi/10.1360/SSI-2021-0321">Paper</a>] [<a href="https://github.com/qinghew/SDGAN">Code</a>] <br /><br /> 
A framework enables cross-domain face translation by integrating two the discriminators into two encoders.</p>
</td></tr></table>
<h2>Service</h2>
<p>I serve as a reviewer for TPAMI, TIP, SIGGRAPH, CVPR, ECCV, TNNLS, TCSVT, ICLR, ACM MM, ICME, MIR, etc.</p>
<h2>Awards</h2>
<ul>
<li><p><b> First Prize (ğŸ¥‡ 1st Place) at the <a href="https://sites.google.com/view/aaai26-cvm/award-results">AAAI CVM 2026 Main Track</a></b></p>
</li>
<li><p><b>China Association for Science and Technology (CAST) Young Talent Support Program - Doctoral Student Special Initiative</b></p>
</li>
<li><p><b>Outstanding Master's Thesis of Tianjin University 2023</b><br /></p>
</li>
<li><p><u>Gold Award of Huawei Ascend AI Innovation Competition 2023</u><br /></p>
</li>
<li><p><u>Third Prize of "Huawei Cup" - the 18th China Postgraduate Mathematical Contest in Modeling 2020</u><br /></p>
</li>
<li><p><b>Outstanding Graduates from Liaoning Province 2020</b><br /></p>
</li>
<li><p><u>First prize (1st place) of the software competition in Rockwell Automation Dalian Software R&D Center 2019</u><br /></p>
</li>
<li><p><u>Third Prize of Chinese College Students Computer Design Contest 2019</u><br /></p>
</li>
<li><p><u>Honorable Mention of The Mathematical Contest in Modeling (MCM) 2019</u><br /></p>
</li>
<li><p>Provincial second prize of &ldquo;NXP&rdquo; Cup National University Students Intelligent Car Race Competition 2019<br /></p>
</li>
<li><p><u>Third Prize of iCAN International Contest of innovation 2018</u><br /></p>
</li>
<li><p><u>Meritorious Winner of The Mathematical Contest in Modeling (MCM) 2018</u><br /></p>
</li>
<li><p>Provincial second prize of &ldquo;TI&rdquo; Cup Liaoning Provincial Undergraduate Electronic Design Contest 2018<br /></p>
</li>
<li><p>Provincial second prize of &ldquo;NXP&rdquo; Cup National University Students Intelligent Car Race Competition 2018<br /></p>
</li>
<li><p>Provincial second prize of National Undergraduate Electronics Design Contest 2017<br /></p>
</li>
</ul>
</div>
</body>
</html>
